
\subsection{Traveling Salesman Problem (TSP)}
\begin{flushleft}
The TSP is a widely studied combinatorial optimization problem because of the wide range of its applications to various instances in industry. The problem describes a challenge of a salesman who wants to visit $m$ distinct cities and then return to home city. The salesman has to determine the order in which he travels to each city that the total distance is minimized while visiting each city (without any repetitions). Summarizing, we can say that the salesman wants to find a tour of minimum length [\ref{TSP_Ali6}]. The problem can be formulated using a complete graph. Each vertex represents a city and each edge represents the path between any two cities. The existence of an edge between every two vertices makes the graph be termed complete. Defining the problem in this manner allows for us to use various graph theory techniques in attempt to find solutions to this problem. 
In essence, the solution to this problem is to find the optimal Hamiltonian circuit. That is a  path through the graph which starts and ends at the same vertex  and visits each vertex exactly once. Although the TSP is conceptually simple, the problem is categorized as an NP-hard problem, as a  result of the difficulty faced in finding an optimal solution to the problem when there are more than 48 cities considered. In a $n$-city problem, any permutation can be a solution to the problem, and as a result there are $(n-1)!$ possible tours or $n!$ possible tours if the home city can be any city in a search space which is asymmetric (for an undirected graph), while it can be evaluated in $\dfrac{1}{2}(n-1)!$ for symmetric (for a directed graph) TSP with a fixed home city [\ref{TSP_Ali6}]. Since the problem is NP-hard, in order to obtain an exact solution to the problem becomes too difficult, thus various approximation algorithms and heuristics are applied to the TSP in order to get results which can assist in finding solutions that may not be optimal, but good approximations to the problem.
\end{flushleft}

\subsubsection{Heuristics applied to the Traveling Salesman problem}
\begin{flushleft}
In \ref{TSP_hueristicsTSP}, various heuristics which are used to solve the symmetric TSP are discussed. We note that heuristics place priority on computation time rather that finding optimal solutions, thus jeopardizing the tour quality. The Held-Karp (HK) lower bound, which produces the lower bound of the optimal solution is a measure taken to analyze the performance of TSP heuristics. The solution to the lower bound can be found in polynomial time by implementing the Simplex method and a polynomial constraint separation algorithm [\ref{hTSP_lowerbound}]. The HK lower bound technique is not feasible for very large sets, however, is a good benchmark when comparing heuristics. 
\end{flushleft}

\subsubsection{Nearest Neighbor}
\begin{flushleft}
This algorithm is one of the very first used to find a solution to the TSP.  It has a performance order of $\mathcal{O}(n^2)$, in which it fast to produce a short traveling path, however, it is not usually the optimal solution. The algorithm starts at a random city and repeatedly finds the nearest unvisited neighbor and goes there. The algorithm terminates when there are no unvisited neighbors. It can be noted that this algorithm's tours are within $25\%$ of the HK lower bound as found in [\ref{hTSP_NN}].
\end{flushleft}

\subsubsection{Greedy approach}
\begin{flushleft}
The Greedy heuristic performs with $\mathcal{O}(n^2log_2(n))$. The algorithm first sorts the edges in ascending order, then selects the shortest edge and adds it to the salesman's route,  as long as it does not create a cycle with less that $n$ edges or increase the degree of any node to more than two. While adhering to the constraint - that the same edge cannot be added twice. The algorithm repeatedly checks the number of edges in the tour after each iteration, however, when we have $n$ edges in the tour, the algorithm terminates. It can be noted that this algorithms tours are within $15-20\%$ of the HK lower bound [\ref{hTSP_NN}].
\end{flushleft}

\subsubsection{Insertion Heuristic}
\begin{flushleft}
In this algorithm, we start with a tour which is a subset of all the cities. The initial sub-tour can often be a triangle or the convex hull or even merely a single edge.  The remaining cities are then inserted by some heuristic. Various heuristics can be can be applied. In each of these heuristics, at each iteration, the cheapest insertion is performed. The algorithm terminates when there are no unvisited nodes. Two particular insertion heuristics are -  Nearest Insertion and Convex Hull, which respectively have the performance orders $\mathcal{O}(n^2)$ and $\mathcal{O}(n^2log_2(n))$ [ \ref{hTSP_NN}].
\end{flushleft}
 
 
\subsubsection{Christofides}
\begin{flushleft}
The Double Minimum Spanning Tree algorithm, which performs with a complexity of $\mathcal{O}(n^2log_2(n))$. The algorithm stipulates that a minimal spanning tree (MST) must be built from the set of all cities, where each edge is duplicated, hence forming an Eulerian cycle (a graph that uses each edge exactly once and starts and ends at the same node) can easily be constructed. The cycle is then traversed without visiting nodes that have been visited. Professor Christofides extended Double Minimum Spanning Tree algorithm which has a  worst-case ratio of $\dfrac{3}{2}$,  and performance time of $\mathcal{O}(n^3)$. The Christofides algorithm builds a minimal spanning tree from the set of all cities. Then it create a minimum-weight matching (MWM) on the set of nodes with an odd degree. The MST and MWM are added together. From the combined graph,  construct an Euler cycle and traverse it without visiting notes that have already been visited. The main difference in these two algorithms is that in this algorithm there is the additional MWM calculation, which is the most time consuming and resulting in a time complexity of $\mathcal{O}(n^3)$. The Christofidesâ€™ algorithm usually is placed $10\%$ above he HK lower bound [\ref{hTSP_NN}].
\end{flushleft}
\begin{flushleft}
The initial solution space is usually constructed by one of these tour construction heuristics. In order to improve these solutions, we can implement various other techniques on these initial solutions. We must note that the performance will now be the combination of these applications. 
\end{flushleft}
\subsubsection{2-Opt and k-opt}
\begin{flushleft}
On a given tour, the 2-Opt algorithm removes two edges from the tour, and reconnects the two paths created such that all constraints are adhered to and making the resulting tour shorter. The algorithm terminates when no 2-Opt improvements can be found. Then the tour can be deemed as 2-optimal. The generalization for this algorithm is denoted as $k$-opt. As $k$ increases, the algorithm takes longer to perform although yielding slight improvements to the 2-Opt heuristic.  In comparison to the lower bound obtained by HK, the 2-Opt heuristic often results in a tour with length less than $5\%$ above the HK bound [\ref{hTSP_NN}]. 
\end{flushleft}
\begin{flushleft}
In order to obtain an optimal solution, alternative methods would have to be implemented in order to move par sub-optimal solutions and perhaps even being liberated from endless local optimal solution cycles. In order to do this, we implement metaheuristics, which are higher-level procedures which have been designed to provide solutions that are considered to be sufficiently good when computation capacity is limited and the solution set is  large.  Metaheuristics do not guarantee global optimum solutions,  which possibly could be obtained through various iterative methods. Metaheuristic formulations are usually approximate and non-deterministic and can be applied to a spectrum of problems by making a few assumptions about the problem. Using these search process, which are guided in some respect, we hope to obtain a solution which is near-optimal. 
\end{flushleft}

\subsubsection{Tabu-Search}
\begin{flushleft}
Various neighborhood searches result in getting stuck at the local optimal solution, when searching among neighboring nodes for a node better then the current. The Tabu-Search is not only restricted to movements which result in positive gain, but is also allowed to make moves that result negative gain. However, this could result in an endless loop of where a move could counteract the previous. A tabu-list is created in order to prevent this from occurring. The list contains all the moves that are not allowed (the \textit{taboo} moves). Once a move is made to a neighbor, the move is now added to the tabu-list, such that the move is never applied again, unless the move improves our current best tour or the tabu-list has been pruned. The main disadvantage of Tabu-Search is that in the light of computation time, it performs poorly with complexity $\mathcal{O}(n^3)$ [\ref{hTSP_NN}]. When comparing the Tabu-Search to the 2-Opt local search, it is documented in [\ref{TSP_hueristicsTSP}] that the length of the tours can be improved using the Tabu-Search then merely applying the standard 2-Opt search.
\end{flushleft}
\subsection{Simulated Annealing}
\begin{flushleft}
The Simulated Annealing (SA) technique was first introduced in 1982 by Kirkpetric. This technique arises from a physics problem based on annealing process in metallurgy. In [\ref{SA_5}] insight is given on how to reshape the mathematical constructions to be applied to a spectrum of optimization problems. SA can be applied to various combinatorial optimization problems, which are considered to be a probabilistic method.  This iterative improvement strategy attempts to perturb some current sub-optimal solution towards a more feasible direction. This technique is considered closely with the Hill Climbing (HC) technique. The HC technique is found to terminate at the solution which are usually the local optimum, rather than the global optimum. Although, SA does not guarantee global optimum solutions, it does provide some probability of not getting trapped at a local optimum solution. The simulated technique can be improved in it's performance time, as it is found to be slow, as a result of it's iterative improvement nature and because of the multiple configurations it visits. Suggested improvements are to incorporate inherently annealing techniques, which are faster performing, and modeling the cost function such that it is more annealable. The convergence of the SA technique can be proven by making simplifications such as imposing the probability to be a Markov chain. When the algorithm is performed multiple times, possibly infinitely many times, then the probability of finding the global optimum is close to one, hence illustrating that the convergence of the SA algorithm is asymptotic in probability. Due to time constraints it may be infeasible to perform the algorithm many times, hence a possible solution to balancing the performance  speed and ensuring quality solutions, is by leaning toward parallel annealing. The SA technique is most applicable to combinatorial optimization problems which have basic structure of a particular configuration space, thus  is it applicable to both the Bin Packing Problem and Traveling Salesman problem.
\end{flushleft}

\begin{flushleft}
In the comparison of the HC technique and SA done in [\ref{SA_HC_7}], it is found that the HC algorithm terminates when there are no neighbor's which are better than the current location. HC is considered incomplete as it usually ends at the local solution, whilst SA techniques incorporate methods to relieve itself from the local optimal solution. Comparing the two techniques, it is found that HC outperforms SA  when random restarts are used, however, when larger search areas are searched, the SA technique achieved better results. The SA technique is found to perform poorly when it's successor is randomly selected and performs it's best when the successor is the best neighbor. Both these techniques are also used in other methods, such as the genetic algorithm as means to find he initial population. 
\end{flushleft}

\begin{flushleft}
An alternative implementation of the SA technique is to use 2-Opt moves to find neighboring solutions [\ref{TSP_hueristicsTSP}].  As a result of employing the 2-Opt technique, the SA implementation performs with $\mathcal{O}(n^2)$ [\ref{hTSP_NN}]. In order to improve this implementation, the list of neighbors which are kept in the 2-Opt and $k$-Opt techniques can be reduced using Tabu-Search techniques. 
\end{flushleft}

\subsubsection{Genetic Algorithms}
\begin{flushleft}
The same concept of GA is considered as described under BPP. Specific to the TSP, the GA randomly generates an initial population of candidate solutions. Candidate solutions (not always all) are then mated to produce offspring, of which some undergo mutation. The fitness value associated with each candidate is considered and the most fit candidates are selected. These candidates then undergo the process until the overall fitness of the population increases. Crossover routines implemented by the GA on the TSP is the measure of fitness. A good measure of fitness is the actual length of the candidate solution [\ref{TSP_hueristicsTSP}]. 
\end{flushleft}


\subsubsection{Branch and Bound}
The  Branch and Bound (BB) method is considered to be an evolutionary process as it first starts with a relaxed version of the problem. BB has good bounds on the value of the optimal solution.  For asymmetric TSP - where the graph representing all the possible routes that can be taken is a directed graph, the depth first search BB technique is implemented. The assignment problem is solved using this method, which can be considered a relaxed form  of the constraints asymmetric TSP. An optimal solution is found to the asymmetric TSP if the solution to the assignment problem is a  complete tour. If it is not a complete tour, we must find a sub tour within the assignment problem solution and eliminate edges from it. We note that this is merely an approximate solution as some constraints have been relaxed. In order to find the optimal solution, we impose the constraints and remove the highest valued edges of the sub tour [\ref{TSP_hueristicsTSP}]. In some instances, the sub tour or the subproblem is not worth further analyzing. These particular cases are when the branch cannot better the value of the relaxed solution from which it originally split, this is the branch's  lower bound or occurs  when a branch is higher than the upper bound. The solution is updated when the relaxed solution of any subproblem produces a feasible solution, then the upper bound is updated with this solution, and we discard the subproblem [\ref{BB_ALI37}].

(TO DO: ADD FIgure for clarity)